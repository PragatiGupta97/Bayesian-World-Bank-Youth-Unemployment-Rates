---
title: "BDA Project"
author: "Dylan, Francesco, Pragati"
output:
  pdf_document:
    toc: yes
    toc_depth: 1
  html_document:
    toc: yes
    toc_depth: '1'
    df_print: paged
  word_document:
    toc: yes
    toc_depth: '1'
bibliography: uni.bib
---



```{r setup, include=FALSE}
# This chunk sets echo = TRUE as default, that is print all code.
# knitr::opts_chunk$set can be used to set other notebook generation options, too.
# include=FALSE inside curly brackets makes this block not be included in the pdf
knitr::opts_chunk$set(echo = TRUE)
```


```{r, results='hide',message=FALSE,include=FALSE}
library(rstan)
library(bayesplot)
library(loo)
library(WVPlots)
library(latticeExtra)
library(sfsmisc)
options(mc.cores = parallel::detectCores(),control = list(max_treedepth = 11))
rstan_options(auto_write = TRUE)
set.seed(123)

```


# 1) Introduction

## Problems

In recent years, both scientists and public are concerning about environmental pollution and toxicity. The level of environmental pollution has increased in years and has had any sign of stopping. Observing this problems, we want to understand more about how pollution level is quantified and predicted. There has been many dataset published relate to this problem and our group decide to choose dataset which relates to aquatic toxicity. This dataset measure the amount of effective substances within the aquatic test sample and measure the toxicity of the sample. However, it is hard to illustrate how toxic a test sample is, especially there hasn't been any general measurement existed yet. In this dataset, they use survivability of *Daphnia* (LC50) as the measurement, which is the concentration that causes death of 50% of *Daphnia* within 48 hours. Lethal concentrations were first converted to molarity and then transformed to a logarithmic scale (–Log mol/L). The predictive value (Quantitative Response) is the level of LC50 that we want to conclude from the dataset. By understanding the value of data, we want to know, whether we can predict the LC50 level knowing the component of substances or not.

This dataset was used to develop quantitative regression QSAR models to predict acute aquatic toxicity towards the fish Pimephales promelas (fathead minnow) on a set of 908 chemicals. The model comprised 8 molecular descriptors: TPSA(Tot) (Molecular properties), SAacc (Molecular properties), H-050 (Atom-centred fragments), MLOGP (Molecular properties), RDCHI (Connectivity indices), GATS1p (2D autocorrelations), nN (Constitutional indices), C-040 (Atom-centred fragments).

This report presents how to detect the "Quantitative Response" using Bayesian Inference. Bayesian modeling provides a principled way to quantify uncertainty and incorporate prior knowledge into the model. What is more, **Stan**’s main inference engine, **Hamiltonian Monte Carlo sampling**, is friendly to diagnostics, which means we can verify whether our inference is reliable. Stan is an expressive probabilistic programing language that abstracts the inference and allows users to focus on the modeling. The resulting code is readable and easily extensible, which makes the modeler’s work more transparent and flexible. @QSAR

## Modeling idea

In this report, We focus on Bayesian inference with MCMC. Bayesian inference gives us a principled quantification of uncertainty and the ability to incorporate domain knowledge in the form of priors, while MCMC is a reliable and flexible algorithm. 

In addition, Stan provides diagnostic tools to evaluate both the inference (e.g. accuracy of the MCMC, convergence of chains) and the model (e.g. posterior predictive checks).

This reports use 3 different type of models i.e

1) Linear Model
2) Hierarchial Model
3) Gaussian Process

Further section explains How to the Stan model was run, Convergence diagnostics,Posterior predictive checks,Model comparison (e.g. with LOO-CV),Predictive performance assessment,Sensitivity analysis with respect to prior choices, all these steps has been performed separately on these 3 models

## Illustrative figure 

Lets visualizing the density plot of each variable in order to check the range of the  variables in the dataset.
```{r}
QSAR <- read.csv(file = 'qsar_aquatic_toxicity.csv')
par(mfrow=c(3,3))
cols = colnames(QSAR)

for(col in cols)
{ 
  plot(density(QSAR[col][,1]), main=col)
}
```

Now lets visualize the pair plot in order to see if there is any collinearity in the data set or not.

```{r}
#PairPlot(QSAR, colnames(QSAR)[1:8], 
#"Pair Plotting of each pair of features", alpha = 0.8,  point_color = "blue")
pairs(QSAR[,1:8], pch = 18,  cex = 0.4, col = "#FC4E07", lower.panel=NULL)
#heatmap(as.matrix(QSAR))
#boxplot.default(QSAR, horizontal = TRUE)
```

# 2) Description of the data and the analysis problem. 

The dataset has been obtained from UCI dataset archives

(https://archive.ics.uci.edu/ml/datasets/QSAR+aquatic+toxicity)

The following table explains the explanatory and the target variables present in the dataset

|feature number|Feature name|Feature Description|
|---|---|---|
| 1 |TSPA|Tot Molecular properties|
| 2 |SAACC|Molecular properties|
| 3 |H050|Atom-centred fragments|
| 4 |MLOGP|Molecular properties|
| 5 |RDCHI|Connectivity indices|
| 6 |GATS1p|2D autocorrelations|
| 7 |nN|Constitutional indices|
| 8 |C040|Atom-centred fragments|
| 9 |Quantitative Response|acute aquatic toxicity|


In order, to analyze the problem in detail, the linear Regression model has been implemented. This has been implemented so that later the bayesian models can be compared with this linear regression model. This model gave us the base estimate and helped in visualizing the data more effectively.

Clearly we can see almost all the variables are statistically significant. 

```{r}
#creating linear model
fullmodel=lm(quantitative_response~TSPA+Saacc+H050+MLOGP+RDCHI+GATS1p+nN+C040, data =QSAR)
summary(fullmodel)
#plot(QSAR$quantitative_response,fullmodel$res,
#ylab="e_bar", xlab="y_bar", main="Residual plot")
#abline(0,0)
```


# 3) Description of the 3 models used

For the sake of this project, we have implemented Linear(Non Hierarchical), Hierarchical model and Gaussian Process

## a) Linear (Non Hierarchical)

In the linear model, quantitative response is normally distributed with mean mu and variance sigma. Mean mu is computed by the dot product between coeffiencients and explanatory variables.
$$
y \sim \mathcal{N}(\mu, \sigma)
$$
where $\mu$ is the predictive value from linear model

$$
\mu = W\times X
$$
which $W$ indicates the vector of weights assign to each value of columns, $X$ indicates the data matrix.

## b) Hierarchical

In the hierarchical model, the data has been categorized according to C040 explanatory variables. C040 represents the number of carbon atoms. After thorough reading of research papers, our team analyzed the C040 maybe be an interesting variable. Therefore hierarchical model has been constructed based on C040, i.e C040 will decide the prior based on C040 variable.

In the hierarchical model, quantitative response is normally distributed with mean mu and variance sigma similar to linear model.

$$
y_{ij} \sim \mathcal{N}(\mu_{i}, \sigma)
$$

However, the major difference here is of $\mu$ and the priors .

$$
\mu_{i} = W_{i}\times X
$$
$$
W_{i} \sim \mathcal{N}(u_{c}, \sigma_{c})
$$

$\u_{c}$ and $\sigma_{c}$ are drawn from prior which share the same value if data rows are in same group. This means mu is dependent on the the coefficient weight which maps to corresponding C040 value.

Here weight is a matrix of dimension NC x (J+1) (number of distinct values of C040 and free value X explanatory variables).

## c) Gaussian Process

In the above section, we use linear model to predict quantitative response. However, since linear model is fairly simple, we want to find a better way of modelling to improve the performance of our predictive model. Therefore, in this section, we will implement Gaussian Process to find a suitable way of modelling.

For given set of data points, there are countless function that can provide a good fit for our dataset. Gaussian Process assigns each possible function a probability and the mean over the probability distribution provides the most probable suitable function for our dataset @gaussian. It can be considered a prior to the probability function $P(f)$, in which $f$ is our modelling function. 
$$
f \sim GP(\mu(x), K(x|\theta))
$$

The parameter of this probability distribution is mean function $\mu$ and covariance function $k$. $k$ is defined as the covariance kernel that calculate the distance between all pairwise data points. It determines the variation of functions of the Gaussian Process. $\theta$ is the parameter of specific kernel.

After that, the observation of Gaussian process is drawn from Normal distribution
$$
y \sim \mathcal{N}(f, \sigma^2)
$$
The kernel that we use in this report is exponentiated quadratic function,
$$
K(x| \alpha, \rho, \sigma)_{i, j} = \alpha^2 exp(-\frac{1}{2\rho^2}\sum_{d=1}^{D}(x_{i,d} - x_{j,d})^2) + \delta_{i,j}\sigma^2 
$$
where $\alpha$, $\rho$, $\sigma$ are hyperparameters defining the covariance function and where $\delta_{i,j}$ is the Kronecker delta function with value 1 if $i=j$ and value 0 otherwise. This test is between the indexes $i$ and $j$, not between values $x_{i}$ and $x_{j}$.


# 4) Informative or weakly informative priors
For this report, we used the following priors

1) Weakly informative prior, very weak: normal(0, 10);
2) Generic weakly informative prior: normal(0, 1);

The recommendations have been followed as per @prior_recom

https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

## Linear Model

$$
W \sim \mathcal{N}(0, 1)
$$

## Hierarchical Model

$$
\mu_{c} \sim \mathcal{N}(0, 1) 
$$
$$
\sigma_{c} \sim \mathcal{N}(0, 1) 
$$

## Gaussian Process Model

For Gaussian Process model, we use suggested prior from the tutorial

https://mc-stan.org/docs/2_19/stan-users-guide/fit-gp-section.html

The priors that I choose for Gaussian Process base on https://mc-stan.org/docs/2_19/stan-users-guide/fit-gp-section.html instruction.

$$
\rho \sim InvGamma(5,5)
$$
$$
\alpha \sim \mathcal{N}(0,1)
$$
$$
\sigma \sim \mathcal{N}(0,1)
$$
$$
\eta \sim \mathcal{N}(0,1)
$$


# 5) Stan code 

## a) Linear (Non Hierarchical)

```{r}
code <- file("linear_model_split.stan")
writeLines(readLines(code))
```


## b) Hierarchical


```{r}
code_hierarchial <- file("hierarchial.stan")
writeLines(readLines(code_hierarchial))
```

## c) Gaussian Process Model

We will implement the above model in Stan. Fix data is declared in data block. $x_{1}$, $y_{1}$ refer to train data, $x_{2}$ refers to test data

```{}
data {
  int <lower=0> J;
  
  // observe data
  int <lower=0> N1;
  vector [J] x1[N1];
  vector [N1] y1;
  
  // test data
  int <lower=0> N2;
  vector [J] x2[N2];
}
```

In the transformed_data code block, we merge the train and test data to calculate covarivance for the whole dataset

```{}
transformed data {
  int<lower=1> N = N1 + N2;
  vector[J] x[N]; //number all
  for (n1 in 1:N1) x[n1] = x1[n1];
  for (n2 in 1:N2) x[N1 + n2] = x2[n2];
}
```

Next, we declare the parameter of the model. Notice that the covariances should be greater than 0. Therefore, we set lower bound of parameters to be 0. We will explicitly define $\eta$ parameter as the latent variable formulation of a GP in Stan. This will be useful for when the outcome is not normal. 

```{}
parameters {
  real <lower=0> rho;
  real <lower=0> alpha;
  real <lower=0> sigma;
  vector[N] eta;
}
```

After that, we transform the parameters as model instruct. In this part, We use the Cholesky parameterized multivariate normal rather than the standard parameterization because it allows us to the cholesky_decompose function which has been optimized for both small and large matrices. This will increase the computational speed of our model.

```{}
transformed parameters {
  vector[N] f;
  {
    matrix[N,N] K = cov_exp_quad(x, alpha, rho) + diag_matrix(rep_vector(square(sigma), N));
    matrix[N,N] L_K = cholesky_decompose(K);
    f = L_K * eta;
  }
}
```

The only thing left is to choose priors and fit the model.

```{}
model {
  // prior
  rho ~ inv_gamma(5, 5);
  alpha ~ normal(0, 1);
  sigma ~ normal(0, 1);
  eta ~ normal(0, 1);
  
  // likelihood
  y1 ~ normal(f[1:N1], sigma);
}
```

We also put predictive result and log likelihood function in generated_quantities code block.

```{}
generated quantities {
  vector[N2] y2;
  vector[N1] log_lik_1;
  
  // predictive assessment
  for(n2 in 1:N2)
    y2[n2] = normal_rng(f[N1 + n2], sigma);
  
  // log likelihood
  for (n1 in 1:N1)
    log_lik_1[n1]= normal_lpdf(y1[n1] | f[n1] ,sigma );
}
```

# 6) How the Stan model was run

## a) Linear (Non Hierarchical)

The Linear model has been run for 1000 interations and with 4 monte carlo chains. The dataset has been divided into test and train sets for predictive checks.


```{r, warning=FALSE, results='hide'}

N_test=20
n=dim(QSAR)[1]
N_train = n-N_test
qr_train=QSAR$quantitative_response[1:N_train]
J=(dim(QSAR)[2]-1)
x_train= QSAR[1:N_train,1:(dim(QSAR)[2]-1)]
#N_test
x_test= QSAR[(N_train+1):n,1:(dim(QSAR)[2]-1)]

qsar_data_check <-list(N_train=N_train,qr_train=qr_train,
                       J=J,x_train=x_train,N_test=N_test, x_test=x_test)

linear_model <-stan(file = 'linear_model_split.stan' ,
                    data = qsar_data_check, chains=4, iter=1000)
params=extract(linear_model, permuted=FALSE, inc_warmup=TRUE)

```


## b) Hierarchical

The Hierarchical model will be run for 2000 iterations and 4 chains. Initially the model was run with default adapt_delta, but there were too many divergences.
So adapt_delta was set to 0.95 in order to make it less divergent.

```{r, warning=FALSE, results='hide'}


n=dim(QSAR)[1]

qr=QSAR$quantitative_response
J=(dim(QSAR)[2]-2)
x= QSAR[,1:(dim(QSAR)[2]-2)]
C040=as.integer(QSAR$C040)
nc = length(unique(QSAR$C040))

qsar_data <-list(N=n,qr=qr,J=J,x=x,C040=C040,nc=nc)

hierarchial <-stan(file = 'hierarchial.stan' , data = qsar_data, chains=4,
                   iter=2000, control=list(adapt_delta=0.95))
#print(hierarchial)
```

## c) Gaussian Process Model

Here, I choose 100 rows of data as train data and next 2 rows of data as test data. The number of iterations per chain is 4000, with the default number of chains is 4.

```{r, warning=FALSE, results = 'hide'}
N2 = 20
N = 120
N1 = N -N2
J = dim(QSAR)[2]-1

x1 = QSAR[1:N1, 1:8]
x2 = QSAR[(N1+1):N, 1:8]
y1 = QSAR[1:N1, 9]
y2 = QSAR[(N1+1):N, 9]

qsar_data <-list(J=J, N1=N1, N2=N2, x1=x1, y1=y1, x2=x2)

gaussian_process <-stan(file = 'gaussian_process.stan' , data = qsar_data, iter = 4000)
```

# 7) Convergence diagnostics 

Here we will discuss 5 types of convergence tests
1) Traceplots
2) $\hat{R}$
3) $n_{eff}$
4) Bulk ESS and Tail ESS
5) Divergences

## Linear (Non Hierarchical)

### 1) Traceplots

The traceplots clearly show that the parameters have converged.

```{r}
traceplot(linear_model, pars=c("alpha","beta"))
```

### 2) $\hat{R}$

```{r}
Rhat_linear= summary(linear_model)$summary[,'Rhat']
print(max(Rhat_linear))
```

From printed output we can see that $\hat{R}$ < 1.01 for all the parameters

### 3) $n_{eff}$

```{r}
neff=summary(linear_model)$summary[,'n_eff']
val=neff/1000 
#print(val)
which(val < 0.01)
```

$\frac{samples}{totalIterations}$ > 0.01 for all parameters, this means samples are not biased and true effect of sample size is not overestimated.

### 4) Bulk ESS and Tail ESS

```{r, results='hide',message=FALSE}
bulk_ess=monitor(extract(linear_model,permute=FALSE, inc_warmup=FALSE))[,'Bulk_ESS']
length(which(bulk_ess < 100))
```

BUlk ESS over 100 for all the parameters i.e reliable

### 5) Divergences

```{r}
get_num_divergent(linear_model)
```

No divergences in the linear model


## Hierarchical 

The plots are seem to be converging.
Only plots for alpha have been added since the plots for beta were too many to be added to this report.
However, we have checked individual convergence for all beta. They seem to converge.

```{r}
traceplot(hierarchial, pars=c("alpha"))
```

### 2) $\hat{R}$

```{r}
Rhat_h= summary(hierarchial)$summary[,'Rhat']
print(max(Rhat_h))
```
From printed output we can see that $\hat{R}$ < 1.03 for all the parameters. This implies Hierarchical model have not converged properly.

### 3) $n_{eff}$

```{r}
neff=summary(hierarchial)$summary[,'n_eff']
val=neff/2000 
#print(val)
length(which(val < 0.01))
```

$\frac{samples}{totalIterations}$ > 0.01 for all parameters, this means samples are not biased and true effect of sample size is not overestimated.

### 4) Bulk ESS and Tail ESS

```{r,results='hide',message=FALSE}
bulk_ess=monitor(extract(hierarchial,permute=FALSE, inc_warmup=FALSE))[,'Bulk_ESS']
length(which(bulk_ess < 100))
```

BUlk ESS  are not reliable.

### 5) Divergences


```{r}
get_num_divergent(hierarchial)
```

Initially the divergences were more than 1000, 
but after setting the **adapt_delta=0.95**, the divergences were reduced.
So we evaluated the hierarchical model was not satisfactory.

##) Gaussian Process Model

### 1) Traceplots

The trace plots clearly show that the parameters have converged.

```{r}
traceplot(gaussian_process, pars=c("alpha","rho", "sigma"))
```

### 2) $\hat{R}$

```{r}
Rhat_linear= summary(gaussian_process)$summary[,'Rhat']
print(max(Rhat_linear))
```
From printed output we can see that $\hat{R}$ < 1.01 for all the parameters

### 3) $n_{eff}$

```{r}
neff=summary(gaussian_process)$summary[,'n_eff']
val=neff/1000 
which(val < 0.01)
```

$\frac{samples}{totalIterations}$ > 0.01 for all parameters, this means samples are not biased and true effect of sample size is not overestimated.

### 4) Bulk ESS and Tail ESS

```{r,results='hide',message=FALSE}
bulk_ess=monitor(extract(gaussian_process,permute=FALSE, inc_warmup=FALSE))[,'Bulk_ESS']
length(which(bulk_ess < 100))
```

### 5) Divergences

```{r}
get_num_divergent(gaussian_process)
```

No divergences in the gaussian process model.


# 8) Posterior predictive checks

In order to check the Posterior, we extract the values of the quantitative response of the last 20 interactions and compare it with the actual quantitative response.

The black density plot is the plot for actual quantitative response.
The red lines are the plot for generated quantitative response.

## Linear (Non Hierarchical)

```{r}
# instead of log, use rng
plot(density(QSAR$quantitative_response),main="Posterior predictive 
     check for simple model")
params<-extract(linear_model)
for (ind in 1980:2000)
{
  lines(density(params$gen_lik[ind,]), col='red');
}

```

##  Hierarchical 

```{r}
# instead of log, use rng
plot(density(QSAR$quantitative_response),main="Posterior predictive check for Hierarchical model")
params<-extract(hierarchial)
for (ind in 3980:4000)
{
  lines(density(params$gen_lik[ind,]), col='red');
}

```

## Gaussian Process Model 

```{r}
plot(density(QSAR[1:N1, 9]),main="Posterior predictive check for 
     gaussian process model", ylim=range(0, 0.4))
params<-extract(gaussian_process)
for (ind in 3980:4000)
{
  lines(density(params$pred_y1[ind,]), col='red');
}
```

We see the posterior check for the linear model are more close to the actual quantitative response. 

# 9) Model comparison 

In this section, we will compute the PSIS_LOO values using the loo library and then compare the models

## Linear (Non Hierarchical)

```{r}
loo_model_linear <- loo(extract_log_lik(linear_model))
print(loo_model_linear)
plot(loo_model_linear, main = "PSIS Diagonostic for linear Model")
```

## Hierarchical 

```{r}
loo_model_hierarchial <- loo(extract_log_lik(hierarchial))
loo_model_hierarchial
plot(loo_model_hierarchial, main = "PSIS Diagonostic for Hierarchical Model")
```

##) Gaussian Process Model

```{r}
loglik <- extract_log_lik(gaussian_process, merge_chains = FALSE)
r_eff <- relative_eff(exp(loglik))
loo_gaussian_model <- loo(loglik, r_eff = r_eff)
print (loo_gaussian_model)
plot(loo_gaussian_model, main = "PSIS Diagonostic for GP Model")
```

From the plot, we see the LOO values of the Linear model are better.


# 10) Predictive performance assessment 

Here the predictive performance is done only for the Linear Model as it performed best for our dataset.

Here, we are comparing the actual quantitative response with the predicted quantitative response.

The plot indicates the actual target variable along with the predicted values with uncertainty.


```{r}
new_params= extract(linear_model)
pred_scores = colMeans(new_params$qr_test)
pred_error = sapply(1:N_test, function(x) sd(new_params$qr_test[,x]))
true_scores = QSAR$quantitative_response[(N_train+1):n]
plot(true_scores,pred_scores,xlim=range(1:10), ylim=range(1:10))
abline(a=0,b=1,lty="dashed")
arrows(true_scores,pred_scores+pred_error,true_scores,
       pred_scores-pred_error, length =0.05, angle = 90, code =3)

```

# 11) Sensitivity analysis 

We now test how sensible our models are to the choice of the priors: we compared the results of Pareto-k-diagnostics for different choices of priors and the comparison is shown in the table below. After using the **weakly informative priors** previously presented, we first tested an even more **wide** and **less informative** prior for each one of the models, then we tested a biased prior, to further investigate the influence the prior has one the models. 

Given the size of the dataset, we could expect that any prior would have little influence on the model, which would resemble instead the shape of the likelihood. This consideration is clearly reflected by the results we obtained (see the table below) for the linear and hierarchical models, while guassian model is definitely sensible to the prior choice.
 
 


# 12) Discussion of issues and potential improvements.

1) Based on our understanding, the hierarchical model was built on the basis of C040 variable. However this model performed worse than the linear model. 
One potential improvement can be building hierarchical model based on some other parameter like H050 or some variable. 
Based on deep understanding on the data, a better way of hierarchical modeling can be constructed.

2) For the Gaussian model which we implemented, we faced the issue of computational time. It took forever to execute our code. Thats why we chose the subset of dataset to work upon. In future maybe with the help of better computation power, we could build a better model.


# 13) Conclusion 

1) One important thing we noticed here was that changing priors didn't change the results significantly. 
This also comes by intuition as the dataset we used in this report was large enough for Bayesian inference. And when the dataset is large enough, priors doesnt make much difference.

2) We concluded that not always the complex model is better. Most often complex models fails to outperform due to lack of domain knowledge. Like in our case the hierarchical model didn't performed good as expected. Maybe it will perform better when grouping method changes, but we have to check that.

The code for model is present at the following Github link, feel free to contribute.

https://github.com/PragatiGupta97/Bayesian-World-Bank-Youth-Unemployment-Rates

 

# 14) Self-reflection of what the group learned while making the project.

1) This project acted like a crash course for the BDA Course for us. This helped us in reviving all the concepts which we already covered in the course but somehow forgotten over time.

2) Developing an end to end project with real dataset helped us in connecting the knowledge which we gained over the course. There were some knowledge gaps in between, but this project helped us in connected all the dots.

3) Overall the experience was quite rich.

# References 

